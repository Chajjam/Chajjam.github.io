---
title: DL Lecture 2 Basics of Neural Network Programming
date: 2021-07-18
categories: Neural_Network
---
**2.1 Binary Classification**\
(x,y): single training sample\
m: number of training examples\
X: [-x<sup>(1)</sup>-;...;x<sup>(m)</sup>] => n<sub>x</sub> * m matrix\
Y: [y<sup>(1)</sup>  ...   y<sup>(m)</sup>] => 1 * m matrix\
\
\
**2.2 Logistic Regression**\
y^ (y hat): P(y=1|x)\
w∈R<sup>n<sub>x</sub></sup> (parameters), b∈R (bias)\
\
y^ = σ(w<sup>T</sup>x) + b\
\
Another notation: (not used in the course)\
x<sub>0</sub> = 1, x∈R<sup>n<sub>x</sub>+1</sup>\
y^ = σ(Θ<sup>T</sup>x)\
Θ = [b ; w]\
\
\
**2.3 Logistic Regression Cost Function**\
Loss (error) function: L(y^,y) =  -(ylog y^ + (1-y)log(1-y^))\
Cost function: J(w,b) = (1/2)∑<sup>m</sup><sub>i=1</sub>L(y^<sup>(i)</sup>,y<sup>(i)</sup>) = -(1/m)∑<sup>m</sup><sub>i=1</sub>[y<sup>(i)</sup>log y^<sup>(i)</sup> + (1-y<sup>(i)</sup>)log(1-y^<sup>(i)</sup>]\
\
Difference between cost function and loss function:\
loss function computes the error for a single training example, while the cost function is the average of the loss functions of the entire training set\
\
\
**2.4 Gradient Descent**\

